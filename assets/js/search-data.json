{
  
    
        "post0": {
            "title": "2. Batch-Gradient Descent for Parameter Learning",
            "content": "A. Linear Regression&lt;/p&gt; Model prediction vectorized form . $ hat{y} = h_{ theta(x)} = theta^T cdot{x} $ MSE cost function for a linear Regression model . Needs to be minimized using gradient descent . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 1. The normal Equation Method . Gives the result directly . $ hat{ theta} = (X^{T} cdot{X})^{-1} cdot{X^{T}} cdot{y} $ where $ hat{ theta} $ is the value if theta that minimizes the cost function. . import numpy as np import matplotlib.pyplot as plt import time np.set_printoptions(precision=20) . np.exp(-999) . np.random.rand(100,1) # generates 100 random numbers between 0 and 1 in form of a vector np.random.randn(100,1) # generates sample of 100 from standard normal distribution. x = 2*np.random.rand(100,1) y = 4 + 3*x + np.random.randn(100,1) # y = 4 + 3*x0 + Gaussian Noise # Think of them as Two linear equations plt.figure(figsize=(12,8)) plt.plot(x,y, &#39;bo&#39;) plt.title(&#39;Randomly Generated Linear Dataset&#39;) plt.show() . # Translates slice objects to concatenation along the second axis. np.c_[np.array([1,2,3]), np.array([4,5,6])] . array([[1, 4], [2, 5], [3, 6]]) . Computing theta using the normal equation . # Use np.linalg to compute inverse of a matrix and dot() method for matrix multiplication. # theta^ = ((X^T.X)^-1).X^T.y # creates a 100*1 column vector np.ones((100,1)) # feature vector x with always x0 = 1 x_b = np.c_[np.ones((100,1)),x] # add x0 = 1 to each row # Transpose using numpy #print(x_b.T) #print(x_b) . dot_1 = x_b.T.dot(x_b) print(dot_1) # inverse using np print(np.linalg.inv(dot_1)) # putting (X^T.X)^-1) together with .X^T.y start_time = time.time() # Theta_best/ learned parameters using normal equation theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y) print(&#39;theta_best n {} n Time Elpased : {}&#39;.format(theta_best, time.time() - start_time)) # Matches really closely with y = 4 + 3*x0 + Gaussian Noise . [[100. 105.6274123936482 ] [105.6274123936482 144.70074549021612]] [[ 0.04367764922618474 -0.03188343675472818 ] [-0.03188343675472818 0.030184812855119657]] theta_best [[6.613381121137904 ] [0.3348113070485745]] Time Elpased : 0.001821756362915039 . Making predictions using theta . y^ = h_theta(x) = theta^T.x . for particular instance [0][2] . np.array([[0],[2]]) x_new = np.array([[0],[2]]) # add x0 = 1 to each instance of feature vector x_new_b = np.c_[np.ones((2,1)), x_new] print(x_new,&#39; n&#39;) print(x_new_b) y_predict = x_new_b.dot(theta_best) y_predict . [[0] [2]] [[1. 0.] [1. 2.]] . array([[4.515565120022579], [9.586861036396929]]) . plt.figure(figsize=(12,8)) plt.plot(x_new, y_predict, &#39;r--&#39;) plt.plot(x,y, &#39;b.&#39;) plt.axis([0,2,0,15]) plt.show() . Computational Complexity of Normal Equation . is O(n^3), for inverting feature matrix. Get very slow for large number of features. So, Gradient Descent is more suitable for large feature maps. . Normal equation is O(m) with regards to number of instances in dataset. Can handle large datasets. . Same thing using Scikit-Learn . from sklearn.linear_model import LinearRegression start_time = time.time() lin_reg = LinearRegression() lin_reg.fit(x,y) print(lin_reg.intercept_, lin_reg.coef_, (time.time()-start_time)) print(x_new) lin_reg.predict(x_new) # No need to padd input, Transpose etc. Handled by sklearn pipeline . [4.236147048957527] [[2.7694719470936566]] 0.0006899833679199219 [[0] [2]] . array([[4.236147048957527], [9.77509094314484 ]]) . import numpy as np np.random.rand(100,1) # generates 100 random numbers between 0 and 1 in form of a vector np.random.randn(100,1) # generates sample of 100 from standard normal distribution. # generating random linear data x = 2*np.random.rand(100,1) y = 4 + 3*x + np.random.randn(100,1) # y = 4 + 3*x0 + Gaussian Noise # feature vector x with always x0 = 1, from linear regression section x_b = np.c_[np.ones((100,1)),x] # add x0 = 1 to each row . # learning rate eta = 0.1 # number of steps for algo. n_iterations =500 # No. of samples m = 100 # randomly initialize cost function parameters theta0 and theta1 theta = np.random.randn(2,1) start_time = time.time() for iterations in range(n_iterations): # This is a single layer, so no back-prop, just accumulating # gradients. gradients = 2/m * x_b.T.dot(x_b.dot(theta) - y) theta_prev = theta theta = theta - eta*gradients if iterations%50 == 0: print(&#39;Gradients at Step : {} are {}&#39;.format(iterations, gradients)) print(&#39;Updated parameters : {}&#39;.format(theta)) if np.array_equal(theta_prev, theta) : print(&#39;Converged, Time Elapsed : {}&#39;.format(time.time()-start_time) ) break . Gradients at Step : 0 are [[-8.940474163505254 ] [-7.3100995983966905]] Updated parameters : [[2.23065940790939 ] [1.9298006117834603]] Gradients at Step : 50 are [[-0.2714985940968554 ] [ 0.23343081217571557]] Updated parameters : [[6.807132030481712 ] [0.027123457170359494]] Gradients at Step : 100 are [[-0.048280434084230245] [ 0.04151086298876905 ]] Updated parameters : [[ 7.442321221833034 ] [-0.5190036022515465]] Gradients at Step : 150 are [[-0.008585680979966934] [ 0.00738185216402258 ]] Updated parameters : [[ 7.555276539288186 ] [-0.6161210535684384]] Gradients at Step : 200 are [[-0.0015267865603945019] [ 0.0013127103954985086]] Updated parameters : [[ 7.575363316448656 ] [-0.6333913923797989]] Gradients at Step : 250 are [[-0.00027150754918976006] [ 0.00023343851165841034]] Updated parameters : [[ 7.578935336126745 ] [-0.6364625664954177]] Gradients at Step : 300 are [[-4.828202656359082e-05] [ 4.151223218184619e-05]] Updated parameters : [[ 7.579570546269155 ] [-0.6370087115682779]] Gradients at Step : 350 are [[-8.5859641706420585e-06] [ 7.3820956458670045e-06]] Updated parameters : [[ 7.579683505312323 ] [-0.6371058322229126]] Gradients at Step : 400 are [[-1.5268369203447208e-06] [ 1.3127536940782393e-06]] Updated parameters : [[ 7.579703592752025 ] [-0.6371231031313676]] Gradients at Step : 450 are [[-2.7151650474621650e-07] [ 2.3344621118415177e-07]] Updated parameters : [[ 7.5797071648895225] [-0.6371261744067831]] . 3. Ordinary Least Squares Method . Minimizes the Residual Sum of errors, $e_{i}^2 = y_i - hat{y_i} $ or for a regression model with a single coefficient, single feature (SLR). $e_{i}^2 = y_i -( beta_0 + beta_1*X)^2.$ | Where the coefficient $$ beta_1 = frac{ text{product of deviations from mean for predictor and target vars.}}{ text{Squared sum of deviation from mean for predictor}} $$ | OR $$ beta_1 = frac{ sum{(x_i- bar{x})(y_i- bar{y})}}{ sum{(x_i- bar{x})}^2} = r( frac{S_y}{S_x})$$ | Y intercept ($ beta_0$) passes through the means, $ bar{x}, bar{y}$. So,$$ beta_0 = bar{y} - hat beta_1 bar{x} $$ Where r is the Pearson&#39;s correlation coefficient. And the Standard deviations of y and x. | Ordinary least squares (OLS) is a non-iterative method that fits a model such that the sum-of-squares of differences of observed and predicted values is minimized. | Minimizes the Residual sum of squares error (RSS), finding this sum is an iterative process.But not finding the parameter. | # lets get mean of the predictor and target vars. x_bar = np.mean(x) y_bar = np.mean(y) num = den =0 # Lets sum up the numberator and denominator seperately. start_time = time.time() # loss for i in range(len(x)): num += (x[i]-x_bar)*(y[i]-y_bar) den += (x[i]-x_bar)**2 #beta_1 = num/den #beta_0 = (y_bar - (beta_1*x_bar)) #print(&#39;Regression Coefficients : {}, {}&#39;.format(beta_0, beta_1)) # First cofficient of regression beta_1 = num/den print(&#39;Beta 1 :&#39;, beta_1) # Y intercept or beta0 beta_0 = (y_bar - (beta_1*x_bar)) print(&#39;Beta 0(y-intercept) :&#39;, beta_0) print(&#39;Time Elapsed {}&#39;.format(time.time() - start_time)) . Beta 1 : [2.7694719470936575] Beta 0(y-intercept) : [4.236147048957527] Time Elapsed 0.0019469261169433594 . # y^ = beta_1*x + beta_0 y_hat = beta_1*x + beta_0 . plt.figure(figsize=(10,8)) plt.scatter(x, y) plt.plot(x, y_hat, &#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x11afad198&gt;] . B. Logistic Regression . Modelling probability of a sample belonging to class Y = 1/True given features x. | Hypothesis Function = h_theta(x) = 1/ 1 + e ^ (- theta ^T (.) x) or sigmoid function. | import numpy as np import matplotlib.pyplot as plt . Modelling logit or log-odds function . log(p(x)/ 1 - p(x)) = p(y) where p(x) is the proabability of a sample belonging to class Y = 1/True given features x. . p_x = np.linspace(0.01, 1 , 100, endpoint=False) y_x = np.log(p_x/ (1 - p_x)) plt.plot(y_x, p_x) . [&lt;matplotlib.lines.Line2D at 0x1104cc908&gt;] . Preparing Iris Dataset for Binary Classification . Using only 2 Features Petal length and width and Binary target Iris Virginica - 1 or 0 - not Iris virginica. . from sklearn.datasets import load_iris import numpy as np iris = load_iris() . x = iris.data y = iris.target . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . y[y!=2] = 0 y[y == 2] = 1 . perm = np.random.permutation(150) x_perm = x[perm] y_perm = y[perm] . x_train, x_test = x_perm[:-20], x_perm[-20:] y_train, y_test = y_perm[:-20], y_perm[-20:] . y_train, y_test = y_train.reshape(-1,1), y_test.reshape(-1,1) . x_train = x_train[: , 2:] x_test = x_test[: , 2:] . plt.plot(x_train[y_train.reshape(-1)== 1][:, 0], x_train[y_train.reshape(-1)== 1][:, 1], &#39;r.&#39;, ) plt.plot(x_train[y_train.reshape(-1)== 0][:, 0], x_train[y_train.reshape(-1)== 0][:, 1], &#39;b.&#39;, ) plt.xlabel(&#39;Petal Length (cm)&#39;) plt.ylabel(&#39;Petal Width (cm)&#39;) plt.legend([&#39;Virginica&#39;, &#39;NonVirginica&#39;]) plt.show() . Logistic Regression Models Probability of Samples belonging to Class 1 and 0. Modelling this Probability Distribution Using Sigmoid Function (Hypothesis Function for Logistic Regression). . def sigmoid(z): return 1/(1 + np.exp(-z)) . def relu(z): return [max(0.0,item) for item in z] . fig, ax = plt.subplots(1,1) ax.plot(np.linspace(-4,4, num = 100), relu(np.linspace(-4,4, num = 100)), label = &#39;Relu&#39;) ax.set_ylim(0, 1) ax.plot(np.linspace(-4,4, num = 100), sigmoid(np.linspace(-4,4, num = 100)), label = &#39;Sigmoid Curve&#39;) ax.plot(-4, sigmoid(-4), &#39;r.&#39;, label = &#39;Min&#39;) ax.plot(0, sigmoid(0), &#39;g.&#39;, label = &#39;Zero&#39;) ax.plot(4, sigmoid(4), &#39;b.&#39;, label = &#39;Max&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x1218ab278&gt; . Log Loss Cost Function . def logistic_loss(y, y_hat): return -(np.mean(y*np.log(y_hat) + (1 - y)*np.log(1 - y_hat))) . &#39;&#39;&#39;We can visualise the loss for: 1. Correct Predictions result in a very small log loss, (y = 1, y_hat = .9999) =0.00010000500033334732 2. Incorrect Predictions result in larger losses, and the negative of this log likelihood needs to be minimised by using updating candidate parameters. (y = 0, y = .9999) = 9.210340371976294 &#39;&#39;&#39; logistic_loss(1, .9999) . 0.00010000500033334732 . Using Batch Gradient Descent - To iteratively minimise Loss. . Log loss is a convex funcion, suitable for Gradient Descent. Update weights and biases. . Initialise Weights and Biases. (B0 and B1, B2 for the two features) (Initiaization to 1). | Set Hyperparameter, Alpha = 0.1. | W = np.ones((2, 1)) b = np.ones((1, 1)) #alpha = 0.0001 alpha = .01 . Training Loop - Calculating Loss and Updating Parameters. . Look up this method of calculating Gradient of losses seperate and bias seperately. . import time tic = time.time() m = len(x_train) epoch_log = [] log_loss_log = [] for epoch in range(750): # Multiplying Input Matrix and Model Coefficients. (Theta^T * x) z = np.matmul(x_train, W) + b # Getting Predicted Probabilities for the entire dataset : h_theta(x). pred_probab = sigmoid(z) # Getting Log Loss for these candidate Coefficients. (Log Likelihood of dataset given coefficients.) log_loss = logistic_loss(y_train, pred_probab) epoch_log.append(epoch) log_loss_log.append(log_loss) # Getting predicted values : actual h_theta(x)-y or (y_hat - y) dz = pred_probab - y_train # Using Derivative of Log Loss: # Getting Gradient of train Dataset for weights. (1/m * X^T [h_theta(x) - y]) # Vectorized Approach dw= 1/m * np.matmul(x_train.T, dz) # Getting Gradient of Train Dataset for bias. db = np.sum(dz) # Updating Parameters/Coefficients Using Gradient Descent Learning Rule and learning rate alpha. W = W - (alpha * dw) b = b - (alpha * db) # Printing stats every multiple of 100 if (epoch % 100) == 0 : print(f&#39;The Loss at Epoch {epoch} is : {log_loss}&#39;) toc = time.time() print(f&#39;Time Taken for Optimisation {(toc-tic)*1000} ms&#39;) . The Loss at Epoch 0 is : 3.124624740966011 The Loss at Epoch 100 is : 0.24313445656634777 The Loss at Epoch 200 is : 0.23295246283788076 The Loss at Epoch 300 is : 0.22440130360601326 The Loss at Epoch 400 is : 0.2170489860763412 The Loss at Epoch 500 is : 0.21061468768766295 The Loss at Epoch 600 is : 0.2049060251839904 The Loss at Epoch 700 is : 0.19978554358819123 Time Taken for Optimisation 119.11988258361816 ms . plt.figure(figsize=(10,8)) plt.plot(epoch_log, log_loss_log) . [&lt;matplotlib.lines.Line2D at 0x12183f780&gt;] . Final Trained Weights . b, W . (array([[-8.527821302164893]]), array([[1.314787798880194 ], [1.2672872840745248]])) . Predictions and F1 Score. . from sklearn.metrics import f1_score # Predictions using final weights, gives probabilities of sample belonging to a class. # z = np.matmul(x_train, W) + b # Then sigmoid (z) # With Threshold value 0.5 preds = [] for i in sigmoid(z): if i&gt;0.5: preds.append(1) else: preds.append(0) f1_score(preds, y_train) . 0.9512195121951219 . Decision Boundary . plt.figure(figsize=(10,8)) plt.scatter(x_train[: , 0], x_train[:, 1], c = y_train.ravel()) ax = plt.gca() # Plotting the Decision Boundary. xvals = np.array(ax.get_xlim()).reshape(-1,1) yvals = -(xvals * W[0][0] + b)/ W[1][0] plt.plot(xvals, yvals) plt.ylim(-1, 3) plt.show() . For Test Set . z = np.matmul(x_test, W) + b # Getting Predicted Probabilities for the entire dataset : h_theta(x). # pred_test_set = sigmoid(z) pred_test = [] for i in sigmoid(z): if i&gt;0.5: pred_test.append(1) else: pred_test.append(0) . f1_score(y_test, pred_test) . 0.9473684210526316 . plt.figure(figsize=(10,8)) plt.scatter(x_test[: , 0], x_test[:, 1], c = y_test.ravel()) ax = plt.gca() # Plotting the Decision Boundary. xvals = np.array(ax.get_xlim()).reshape(-1,1) yvals = -(xvals * W[0][0] + b)/ W[1][0] plt.plot(xvals, yvals) plt.ylim(-1, 3) plt.show() . Convolutions with OpenCV and Python . an (image) convolution is simply an element-wise multiplication of two matrices followed by a sum. | Take two matrices (which both have the same dimensions). | Multiply them, element-by-element (i.e., not the dot-product, just a simple multiplication). | Sum the elements together. | Kernels can be an arbitrary size of M x N pixels, provided that both M and N are odd integers. . Note: Most kernels you’ll typically see are actually square N x N matrices. . We use an odd kernel size to ensure there is a valid integer (x, y)-coordinate at the center of the image: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; convolution. . In image processing, a convolution requires three components: . An input image. A kernel matrix that we are going to apply to the input image. An output image to store the output of the input image convolved with the kernel. Convolution is simply the sum of element-wise matrix multiplication between the kernel and neighborhood that the kernel covers of the input image. . Before we continue, it’s important to understand that the process of “sliding” a convolutional matrix across an image, applying the convolution, and then storing the output will actually decrease the spatial dimensions of our output image. . Why is this? . Recall that we “center” our computation around the center (x, y)-coordinate of the input image that the kernel is currently positioned over. This implies there is no such thing as “center” pixels for pixels that fall along the border of the image. The decrease in spatial dimension is simply a side effect of applying convolutions to images. Sometimes this effect is desirable and other times its not, it simply depends on your application. . However, in most cases, we want our output image to have the same dimensions as our input image. To ensure this, we apply padding (Lines 16-19). Here we are simply replicating the pixels along the border of the image, such that the output image will match the dimensions of the input image. . Other padding methods exist, including zero padding (filling the borders with zeros — very common when building Convolutional Neural Networks) and wrap around (where the border pixels are determined by examining the opposite end of the image). In most cases, you’ll see either replicate or zero padding. . from skimage.exposure import rescale_intensity import numpy as np import cv2 import argparse import matplotlib.pyplot as plt # custom convolve method def convolve(image, kernel): # get the dimensions of the image and kernel (iH,iW) = image.shape[:2] # The slicing converts it to greyscale, slicing to two dimensions. (kH,kW) = kernel.shape[:2] #print(iH,iW,&#39; n&#39;) #print(kH,kW,&#39; n&#39;) # &quot;pad&quot; the borders of the input image so the spatial # size (i.e., width and height) are not reduced pad = (kW-1)//2 #cv2.copyMakeBorder(src, top, bottom, left, right, borderType, value) image = cv2.copyMakeBorder(image, pad, pad, pad, pad, cv2.BORDER_REPLICATE) # allocate memory for the output image, taking care to output = np.zeros((iH,iW), dtype = &#39;float32&#39;) ### apply the actual convolution to our image # loop over the input image, &quot;sliding&quot; the kernel across # each (x, y)-coordinate from left-to-right and top to # bottom for y in np.arange(pad, iH+ pad): for x in np.arange(pad, iW + pad): # extract the ROI of the image by extracting the # *center* region of the current (x, y)-coordinates # dimensions roi = image[y - pad:y + pad + 1, x - pad:x + pad + 1] #print(roi.shape) # perform the actual convolution by taking the # element-wise multiplicate between the ROI and # the kernel, then summing the matrix k = (roi * kernel).sum() output[y - pad, x - pad] = k # rescale the output image to be in the range [0, 255] output = rescale_intensity(output, in_range=(0, 255)) output = (output * 255).astype(&quot;uint8&quot;) # return the output image return output #im = np.random.rand(4,4) #plt.imshow(im, cmap = &#39;gray&#39;) #print(im) # kernels have to of arbitrary odd sizes #kl = np.ones((5,5)) #im.shape[:2] #res = convolve(im,kl) #plt.imshow(im, cmap=&#39;gray&#39;) #plt.imshow(res, cmap=&#39;gray&#39;) . #ap = argparse.ArgumentParser() #ap.add_argument(&quot;-i&quot;, &quot;--image&quot;, required=True, # help=&quot;path to the input image&quot;) #args = vars(ap.parse_args()) # construct average blurring kernels used to smooth an image smallBlur = np.ones((7, 7), dtype=&quot;float&quot;) * (1.0 / (7 * 7)) largeBlur = np.ones((21, 21), dtype=&quot;float&quot;) * (1.0 / (21 * 21)) # construct a sharpening filter sharpen = np.array(( [0, -1, 0], [-1, 5, -1], [0, -1, 0]), dtype=&quot;int&quot;) # construct the Laplacian kernel used to detect edge-like # regions of an image laplacian = np.array(( [0, 1, 0], [1, -4, 1], [0, 1, 0]), dtype=&quot;int&quot;) # construct the Sobel x-axis kernel sobelX = np.array(( [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]), dtype=&quot;int&quot;) # construct the Sobel y-axis kernel sobelY = np.array(( [-1, -2, -1], [0, 0, 0], [1, 2, 1]), dtype=&quot;int&quot;) . # to apply using both our custom `convole` function and # OpenCV&#39;s `filter2D` function kernelBank = ( (&quot;small_blur&quot;, smallBlur), (&quot;large_blur&quot;, largeBlur), (&quot;sharpen&quot;, sharpen), (&quot;laplacian&quot;, laplacian), (&quot;sobel_x&quot;, sobelX), (&quot;sobel_y&quot;, sobelY) ) . image_path = &#39;wall/6.jpg&#39; image = cv2.imread(image_path) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) plt.imshow(gray,cmap=&#39;gray&#39;) plt.show() . for (kernelName, kernel) in kernelBank: # apply the kernel to the grayscale image using both # our custom `convole` function and OpenCV&#39;s `filter2D` # function print(&quot;[INFO] applying {} kernel&quot;.format(kernelName)) convoleOutput = convolve(gray, kernel) opencvOutput = cv2.filter2D(gray, -1, kernel) # show the output images cv2.imshow(&quot;original&quot;, gray) cv2.imshow(&quot;{} - convole&quot;.format(kernelName), convoleOutput) cv2.imshow(&quot;{} - opencv&quot;.format(kernelName), opencvOutput) cv2.waitKey(0) cv2.destroyAllWindows() . Decision Trees(WIP) . Gini Impurity Is a measure of purity(all samples belonging to the same class) of a node in a decision tree. Scikit-learn uses CART algo. which can split the samples based on Gini-Impurity or Entropy(Information theory). For the ith node and k classes, $$ G_i = 1 - sum_{k=1}^n{P_{i,k}}^2$$ . where P_i,k is the ratio of class k instances among the training instances in the ith node. . from sklearn.datasets import make_classification import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score, classification_report . feats, labels = make_classification(n_samples=500, n_features=4, n_classes=2) . feats.shape . (500, 4) . labels.shape . (500,) . def split_level(feats, labels): &quot;&quot;&quot;TO BE CONTINUED&quot;&quot;&quot; # Top level. # lets get the dict with all feats and lowest cost ass. cost_level = level_feat_select(feats, labels) # whichever feature had the lowest cost split the dataset into compartments acc. to that print(cost_level) best_split_feat = np.array(cost_level[&#39;lowest_cost&#39;]).argmin() # go to next level with subset of data # feat_num where left is less than . . def level_feat_select(feats, label): # pass each feature one by one. # keep record of which feature had lowest gini impurity. # use that feature first, then dispatch another feature. # keep index index of feat that has lowest cost split_order = {&#39;feat_num&#39;: [], &#39;feat_value&#39;:[], &#39;lowest_cost&#39;:[]} for ith_feat in range(feats.shape[-1]): print(&#39; n&lt;&lt;&lt;&lt;EVAL NEW FEAT&lt;&lt;&lt;&lt;&lt; n&#39;) # copy the ith feature. temp = feats[:,ith_feat].copy() # call split into compartments which returns the value of ith feature with lowest cost. cost_split_feat = split_into_compartments(temp, label) print(&#39;For {} feature, lowest cost {} and feat value {}&#39;.format(ith_feat,cost_split_feat[&#39;cost&#39;],{cost_split_feat[&#39;feat_value&#39;]})) split_order[&#39;feat_num&#39;].append(ith_feat) split_order[&#39;feat_value&#39;].append(cost_split_feat[&#39;feat_value&#39;]) split_order[&#39;lowest_cost&#39;].append(cost_split_feat[&#39;cost&#39;]) return split_order . def split_into_compartments(single_feat, labels): # for each feature, compute the optimal split values that # reduces the gini impurity # also have to return which side to split on lext or right. #sort_index = np.argsort(feats) # cost function : # J(k, tk) = m_left/m Gleft + m_right/m Gright # find t, tk that minimizes the cost function. cost_split = {} cost_split[&#39;cost&#39;] = np.inf for index,feat_val in enumerate(single_feat): print(f&#39;For value of feat : {feat_val}:&#39;) #[left subset] # All feat values lower than the current selected threshold values, # the labels have to be indexed by the same # all the indexes where feature value is less than or equal to the selected value. indexes_left = np.argwhere(single_feat[:] &lt;= feat_val) # end those labels to left subset split_1_labels = labels[indexes_left] print(f&#39; n[LEFT SUBSET] No of instances : {len(split_1_labels)}&#39;) gini_split_1 = gini_impurity_node(split_1_labels) print(f&#39;Gini_impurity : {gini_split_1}&#39;) #[right subset] # all the indexes where feature value is more than the selected value. indexes_right = np.argwhere(single_feat[:] &gt; feat_val) split_2_labels = labels[indexes_right] print(f&#39; n[Right SUBSET] No of instances : {len(split_2_labels)}&#39;) gini_split_2 = gini_impurity_node(split_2_labels) print(f&#39;Gini_impurity : {gini_split_2}&#39;) # J(k, tk) = m_left/m Gleft + m_right/m Gright cost = ((len(split_1_labels)/len(labels)) * gini_split_1) + ((len(split_2_labels)/len(labels)) * gini_split_2) print(f&#39; nCost : {cost}&#39;) if cost &lt; cost_split[&#39;cost&#39;]: cost_split[&#39;cost&#39;] = cost cost_split[&#39;feat_value&#39;] = feat_val print(f&#39;Purer subsets found&#39;) print(&#39; n&#39;) return cost_split . def gini_impurity_node(labels_split): # for the given value of the feature what is the # gini impurity of the node gini_imp_i_split = 0 for class_k in np.unique(labels_split): # Pi,k ratio of instances of class k in the ith node # Ik/Ti ratio = sum(labels_split == class_k)/len(labels_split) ratio = ratio ** 2 print(f&#39;For class {class_k}, ratio Pik^2 : {ratio}&#39;) gini_imp_i_split = gini_imp_i_split + ratio return 1 - gini_imp_i_split . # what my decision tree learned basically plt.scatter(feats[:,1], feats[:,2], c = labels) plt.axvline(0.15551047755399616) . &lt;matplotlib.lines.Line2D at 0x12222c0f0&gt; . preds= np.empty_like(labels) preds[np.argwhere(feats[:,1] &gt; 0.15551047755399616)] = 1 preds[preds!=1] = 0 . print(accuracy_score(preds, labels)) print(classification_report(preds, labels)) . 0.896 precision recall f1-score support 0 0.83 0.96 0.89 216 1 0.96 0.85 0.90 284 accuracy 0.90 500 macro avg 0.90 0.90 0.90 500 weighted avg 0.91 0.90 0.90 500 . &lt;/div&gt; .",
            "url": "https://dnanadev.github.io/Personal_Blog/2020/10/27/Numpy_ML.html",
            "relUrl": "/2020/10/27/Numpy_ML.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Linear and Non-linear Classifiers . Logistic regression is a linear classifier. Decision tree is the simplest non-linear classifier. Real datasets are never linearly separable still using logistic regression is advisable as certain features might be able to linearly separate the data. . import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier . def make_meshgrid(x, y, h=.02): x_min, x_max = x.min() - 1, x.max() + 1 y_min, y_max = y.min() - 1, y.max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) return xx, yy . def plot_contours(ax, clf, xx, yy, **params): Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) out = ax.contourf(xx, yy, Z, **params) return out . # AND of two binary vars #non-linear dataset: #X_OR nonlinear function of two binary vars # lets start with binary random variables x1 = np.random.binomial(n = 1, p = 0.5, size = 100).reshape(-1, 1) x2 = np.random.binomial(n = 1, p = 0.5, size = 100).reshape(-1, 1) # feature vector X_New = np.hstack((x1,x2)) # and of the input as your label with AND Gate y_new = X_New[:,0] &amp; X_New[:,1] x_train,x_test, y_train, y_test = train_test_split(X_New, y_new, test_size = 0.3, shuffle= True) xx, yy = make_meshgrid(x1, x2) reg = LogisticRegression() reg.fit(x_train, y_train.ravel()) print(reg.score(x_train, y_train.ravel())) print(reg.score(x_test, y_test.ravel())) fig, ax = plt.subplots() # title for the plots title = (&#39;Decision surface of Logistic regression with AND&#39;) # Set-up grid for plotting plot_contours(ax, reg, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8) ax.scatter(x1, x2, c=y_new, cmap=plt.cm.coolwarm, s=20, edgecolors=&#39;k&#39;) ax.set_ylabel(&#39;y label here&#39;) ax.set_xlabel(&#39;x label here&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) ax.legend() plt.show() # Xor # np.logical_and(X_New[:,0], X_New[:,1]) y_xor = ( X_New[:,0] &amp; ~X_New[:,1]) | ( ~X_New[:,0] &amp; X_New[:,1]) x_train,x_test, y_train, y_test = train_test_split(X_New, y_xor, test_size = 0.3, shuffle= True) reg.fit(x_train, y_train.ravel()) print(reg.score(x_train, y_train.ravel())) print(reg.score(x_test, y_test.ravel())) xx, yy = make_meshgrid(x1, x2) fig, ax = plt.subplots() # title for the plots title = (&#39;Decision surface of Logistic regression with XOR&#39;) # Set-up grid for plotting plot_contours(ax, reg, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8) ax.scatter(x1, x2, c=y_xor, cmap=plt.cm.coolwarm, s=20, edgecolors=&#39;k&#39;) ax.set_ylabel(&#39;y label here&#39;) ax.set_xlabel(&#39;x label here&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) ax.legend() plt.show() . No handles with labels found to put in legend. . 1.0 1.0 . No handles with labels found to put in legend. . 0.8142857142857143 0.8666666666666667 . clf = DecisionTreeClassifier() clf.fit(x_train, y_train) print(clf.score(x_train, y_train)) print(clf.score(x_test, y_test)) fig, ax = plt.subplots() # title for the plots title = (&#39;Decision surface of Decision Tree with XOR&#39;) # Set-up grid for plotting plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8) ax.scatter(x1, x2, c=y_xor, cmap=plt.cm.coolwarm, s=20, edgecolors=&#39;k&#39;) ax.set_ylabel(&#39;y label here&#39;) ax.set_xlabel(&#39;x label here&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) ax.legend() plt.show() from sklearn.tree import plot_tree plot_tree(clf) . No handles with labels found to put in legend. . 1.0 1.0 . [Text(167.4, 181.2, &#39;X[0] &lt;= 0.5 ngini = 0.493 nsamples = 70 nvalue = [39, 31]&#39;), Text(83.7, 108.72, &#39;X[1] &lt;= 0.5 ngini = 0.5 nsamples = 37 nvalue = [19, 18]&#39;), Text(41.85, 36.23999999999998, &#39;gini = 0.0 nsamples = 19 nvalue = [19, 0]&#39;), Text(125.55000000000001, 36.23999999999998, &#39;gini = 0.0 nsamples = 18 nvalue = [0, 18]&#39;), Text(251.10000000000002, 108.72, &#39;X[1] &lt;= 0.5 ngini = 0.478 nsamples = 33 nvalue = [20, 13]&#39;), Text(209.25, 36.23999999999998, &#39;gini = 0.0 nsamples = 13 nvalue = [0, 13]&#39;), Text(292.95, 36.23999999999998, &#39;gini = 0.0 nsamples = 20 nvalue = [20, 0]&#39;)] .",
            "url": "https://dnanadev.github.io/Personal_Blog/2020/10/27/Classification_Decision_Functions.html",
            "relUrl": "/2020/10/27/Classification_Decision_Functions.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "R-squared: Statistical and Machine Learning interpretation",
            "content": "R-squared . R-squared ($R^2$), also known as the coefficient of determination is a measure to evaluate the overall fit of a linear model i.e how close the fitted regression line is to the observed data. . One of the most common metrics for evaluating the performance of regression models, also is a point of confusion and often leads to misinterpreted results. I believe this is due to the difference in explanation of the metric from statistics and machine learning sources. These . In this post we&#39;ll derive R-squared using both approaches and also implement them to check the performance of a model on a toy dataset. The Boston House prices dataset and a Random Forest Regressor are used to illustrate the concepts in the code snippets. . Statistical Definition: . Defines R-squared as the proportion of variance explained, meaning the proportion of variance of the target y that is explained by the model. It is also defined as comparing the predictions of a model to a simple null model, one that always predicts the mean of the target variable. . The R-squared is defined as a proportion with a range of [0, 1] where a model that perfectly explains the variation in the target has a score of 1 and a score of 0 represents a model that explains none of the variation in the target. . However, these definitions don’t talk about how R-squared can be negative if the fit of the model is worse than that of the null model. In fact, using R-squared in the machine learning context where we care about generalising i.e. predicting on unseen or out of sample data the R-squared is often negative and problematic. . For a model $h_ theta$, consider the Random variables $y$ (Target) and $ hat{y}$ (predicted values by model). . The Residual of the error is the difference between the actual value of the target and the predictions of the model. . $$e_i = y_i - hat{y_i}$$ . The variance of a random variable x is the measure of the spread of its distribution in the case of a population or more generally it is the measure of how far on average the observed value of a random variable is from its expected value. . $$Var(x) = E[(x - E(x))^2]$$ . which for a population with n data points: . $$ sigma^2 = frac{1}{n} sum{(x_i - bar{x})^2}$$ . and for a sample, the unbiased estimate of the variance: . $$S^2 = frac{1}{n-1} sum{(x_i - bar{x})^2} $$ . and, R-squared from its definition as a proportion of explained variance is 1 minus the proportion of unexplained variance, . $$ R^2 = 1 - frac{S_{e}^{2}( text{Error from fitted model})}{S_{y}^{2}( text{From data points})} $$ . When and why is R-squared negative. . In the statistical learning concepts we do not not care about generalising to unseen data. In the case of machine learning the model&#39;s fit can be arbitrarily worse than the null model and the R-squared will be negative as a result. For the training set the R-squared in bounded by [0, 1] but not when we present the model unseen data. . R^2 for null model : 0.0 MSE for null model : 9.757620233144245 . Shifting the prediction of mean by a value of -10. . R^2 for null model : -1.0502971264840726 MSE for null model : 13.97179847457964 . Machine Learning Definition: . Focuses on the definition of the error estimate in terms of the sum of squared residual errors (SSE), the mean squared error (MSE) is simply the SSE divided by the total samples of the data. . Ultimately gives us the form : . $$R^2 = 1 - frac{MSE_{ text{Error from fitted model}}}{MSE_{ bar{y}( text{From data points})} }$$ . To conclude, The R2 score, or coefficient of determination is a common regression, which measures how well a model performs relative to a simple mean of the target values. It is often thought of as the amount of variance explained by a model. Where, R2 = 1 indicates a perfect match, R2 = 0 indicates the model does no better than simply taking the mean of the data, and negative values mean even worse models. . Illustrating the out-of-sample R^2 of predictions made by a Random Forest Regressor : . RF Train R^2 : 0.9813427844398096 Out of bag estimate R^2 : 0.850591725350667 RF Validation R^2 : 0.8183672893142674 . Manual implementation of Coefficient of determination R^2 . $$R^2 = 1 - frac{S_{e}^{2}( text{Uses fitted line})}{S_{y}^{2}( text{From data points})}$$ where $S_{e}^{2}$ is the sample variance of the residuals and $S_{y}^{2}$ is the sample variance of the outputs. . resids = preds - y_test # Using sample variance of residuals to calculate R^2. 1 - (np.var(resids, ddof=1))/ (np.var(y_test, ddof=1)) . 0.8209610683012851 . clf_rf.score(x_test, y_test) . 0.8183672893142674 . 1 - mean_squared_error(y_test, preds)/ mean_squared_error(y_test, np.repeat(np.mean(y_test), y_test.shape[0])) . 0.8183672893142674 . Limitations of R^2 . R2 is a proportion but can be misleading as a metric for evaluating performance of a model on a task. Another problem is that it is problematic for time-series data etc. . The threshold for a &quot;good&quot; R-squared value is highly dependent on the particular domain. . R-squared is more useful as a tool for comparing models. | R-squared will always increase as you add more features to the model, even if they are unrelated to the response. | As such, using R-squared as a model evaluation metric can lead to overfitting. | Adjusted R-squared is an alternative that penalizes model complexity (to control for overfitting), but it generally under-penalizes complexity. | As well, R-squared depends on the same assumptions as p-values, and it&#39;s less reliable if those assumptions are violated. (Linear models rely upon a lot of assumptions (such as the features being independent), and if those assumptions are violated (which they usually are), p-values are less reliable.) | . Adjusted R^2 . is an alternative to R^2 that penalizes model complexity (to control for overfitting), but it generally under-penalizes complexity. Defined as : $$ text{Adj } R^2 = 1-(1-R^2)* frac{(n-1)}{(n-p-1)} $$ . Where n = number of sample size , p = number of independent variables . def adj_r2_score(r2, x_test): &quot;&quot;&quot;Better to extend r2&quot;&quot;&quot; adj_r2 = 1 - (1 - r2) * ((x_test.shape[0] - 1)/(x_test.shape[0] - x_test.shape[1] - 1)) return adj_r2 adj_r2_score(clf_rf.score(x_test, y_test), x_test) . 0.8012569614960462 . More reliable Evaluation metrics for regression problems . MSE and MAE and other error metrics are more suitable error metrics used in ML as the results are more directly comparable given the same scale. . MSE is less robust to ouliers in target compared to MAE. . MAE is the easiest to understand, because it&#39;s the average error. | MSE is more popular than MAE, because MSE &quot;punishes&quot; larger errors, which tends to be useful in the real world. | RMSE is even more popular than MSE, because RMSE is interpretable in the &quot;y&quot; units. | . Time-Series Metrics . from https://otexts.com/fpp2/accuracy.html: A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. . Mean Absolute Percentage Error (MAPE): this is the same as MAE but is computed as a percentage, which is very convenient when you want to explain the quality of the model to management, [0,+∞) . $MAPE = frac{100}{n} sum limits_{i=1}^{n} frac{|y_i - hat{y}_i|}{y_i}$ .",
            "url": "https://dnanadev.github.io/Personal_Blog/regression/evaluation%20metric/2020/10/26/Regression_Metrics.html",
            "relUrl": "/regression/evaluation%20metric/2020/10/26/Regression_Metrics.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://dnanadev.github.io/Personal_Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://dnanadev.github.io/Personal_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dnanadev.github.io/Personal_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}